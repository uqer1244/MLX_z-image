import argparse
import os
import json
import torch
import mlx.core as mx
import mlx.nn as nn
from safetensors.torch import load_file as load_pt_file
import numpy as np
import shutil

# â— ì¤‘ìš”: ê°™ì€ í´ë”ì— ìžˆëŠ” mlx_text_encoder.pyì—ì„œ í´ëž˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
try:
    from mlx_text_encoder import TextEncoderMLX
except ImportError:
    print("âŒ Error: 'mlx_text_encoder.py' not found. Please place it in the same directory.")
    exit(1)


def main():
    parser = argparse.ArgumentParser(description="Convert & Quantize Text Encoder to MLX (4-bit)")
    parser.add_argument("--src_path", type=str, default="Z-Image-Turbo/text_encoder",
                        help="Path to PyTorch model folder")
    parser.add_argument("--dest_path", type=str, default="Z-Image-Turbo-MLX-TextEncoder-4bit",
                        help="Output path")
    parser.add_argument("--group_size", type=int, default=32,
                        help="Quantization group size (Recommended: 32 for quality, 64 for size)")
    args = parser.parse_args()

    print(f"ðŸš€ Starting 4-bit Quantization Conversion")
    print(f"   Source: {args.src_path}")
    print(f"   Target: {args.dest_path}")
    print(f"   Group Size: {args.group_size}")

    os.makedirs(args.dest_path, exist_ok=True)

    # 1. Config ë¡œë“œ ë° ëª¨ë¸ ì´ˆê¸°í™”
    config_path = os.path.join(args.src_path, "config.json")
    if not os.path.exists(config_path):
        print(f"âŒ Error: Config not found at {config_path}")
        return

    print("\n[1/4] Loading Configuration...")
    with open(config_path, "r") as f:
        config = json.load(f)

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (êµ¬ì¡°ë§Œ ìƒì„±ë¨)
    model = TextEncoderMLX(config)
    print("   âœ… Model initialized.")

    # 2. PyTorch ê°€ì¤‘ì¹˜ ë¡œë“œ ë° í†µí•©
    print("\n[2/4] Loading & Converting Weights (This may take memory)...")

    index_path = os.path.join(args.src_path, "model.safetensors.index.json")
    collected_weights = {}

    if os.path.exists(index_path):
        # ìƒ¤ë”©ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        with open(index_path, "r") as f:
            index_data = json.load(f)
        shard_files = sorted(list(set(index_data["weight_map"].values())))

        for i, filename in enumerate(shard_files):
            file_path = os.path.join(args.src_path, filename)
            print(f"   Processing shard {i + 1}/{len(shard_files)}: {filename}...")

            pt_weights = load_pt_file(file_path)
            for k, v in pt_weights.items():
                # PyTorch Tensor -> Numpy -> MLX Array
                if isinstance(v, torch.Tensor):
                    # bfloat16 to float32 conversion for numpy compatibility
                    val_np = v.float().numpy()
                else:
                    val_np = v

                # MLXë¡œ ë³€í™˜ (ì•„ì§ì€ FP16/BF16 ìƒíƒœ)
                collected_weights[k] = mx.array(val_np).astype(mx.bfloat16)

            del pt_weights  # ë©”ëª¨ë¦¬ í™•ë³´
            if hasattr(mx, "clear_cache"): mx.clear_cache()
    else:
        # ë‹¨ì¼ íŒŒì¼ ë¡œë“œ
        single_path = os.path.join(args.src_path, "model.safetensors")
        print(f"   Processing single file: model.safetensors...")
        pt_weights = load_pt_file(single_path)
        for k, v in pt_weights.items():
            if isinstance(v, torch.Tensor):
                val_np = v.float().numpy()
            else:
                val_np = v
            collected_weights[k] = mx.array(val_np).astype(mx.bfloat16)

    # 3. ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ ë° ì–‘ìží™”
    print(f"\n[3/4] Quantizing to 4-bit (Group Size: {args.group_size})...")

    # ìˆ˜ì§‘í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ì£¼ìž…
    model.load_weights(list(collected_weights.items()))
    del collected_weights  # ë©”ëª¨ë¦¬ í•´ì œ
    mx.eval(model.parameters())

    # ðŸ”¥ í•µì‹¬: MLX ë‚´ìž¥ ì–‘ìží™” í•¨ìˆ˜ ì‹¤í–‰
    # Linear ë ˆì´ì–´ë“¤ì„ QuantizedLinearë¡œ êµì²´í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì••ì¶•í•¨
    nn.quantize(model, bits=4, group_size=args.group_size)

    print("   âœ… Quantization applied successfully.")

    # 4. ì €ìž¥
    print("\n[4/4] Saving Quantized Model...")

    # ê°€ì¤‘ì¹˜ ì €ìž¥ (MLX í¬ë§· - safetensors)
    weights_path = os.path.join(args.dest_path, "model.safetensors")
    model.save_weights(weights_path)
    print(f"   âœ… Weights saved to {weights_path}")

    # Config ë³µì‚¬
    with open(os.path.join(args.dest_path, "config.json"), "w") as f:
        json.dump(config, f, indent=4)
    print("   âœ… Config saved.")

    # (ì˜µì…˜) Tokenizer íŒŒì¼ë“¤ì´ ìžˆë‹¤ë©´ ë³µì‚¬
    tokenizer_files = ["tokenizer.json", "tokenizer_config.json", "vocab.json", "merges.txt", "special_tokens_map.json"]
    copied_count = 0
    for t_file in tokenizer_files:
        src = os.path.join(args.src_path, t_file)
        if os.path.exists(src):
            shutil.copy2(src, os.path.join(args.dest_path, t_file))
            copied_count += 1

    if copied_count > 0:
        print(f"   âœ… Copied {copied_count} tokenizer files.")

    print(f"\nðŸŽ‰ All Done! Model saved to: {args.dest_path}")
    print(f"ðŸ’¡ Usage in Pipeline: remove 'nn.quantize(...)' and just load this model.")


if __name__ == "__main__":
    main()